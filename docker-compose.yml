# Docker Compose para Airflow + Kedro Pipeline
version: '3.8'

services:
  # ============================================
  # Servicio: Kedro Pipeline Container
  # ============================================
  kedro-pipeline:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: ml-letterboxd-pipeline
    volumes:
      - ./data:/app/data
      - ./logs:/app/logs
      - ./src:/app/src:ro  # Montar código fuente como solo lectura
      - ./conf:/app/conf:ro  # Montar configuración
      - ./dags:/app/dags:ro  # Montar DAGs para referencia
    environment:
      - KEDRO_ENV=base
      - AIRFLOW_HOME=/opt/airflow
      - PYTHONUNBUFFERED=1
    networks:
      - ml-network
    # Mantener contenedor corriendo para que Airflow pueda ejecutar comandos
    command: tail -f /dev/null
    restart: unless-stopped
    mem_limit: 8g
    mem_reservation: 4g
    healthcheck:
      test: ["CMD", "python", "-c", "import kedro; print('Kedro OK')"]
      interval: 30s
      timeout: 10s
      retries: 3

  # ============================================
  # Servicio: Airflow Web Server
  # ============================================
  airflow-webserver:
    image: apache/airflow:2.7.0
    container_name: airflow-webserver
    depends_on:
      - airflow-init
    volumes:
      - ./dags:/opt/airflow/dags
      - ./logs:/opt/airflow/logs
      - ./plugins:/opt/airflow/plugins
      - /var/run/docker.sock:/var/run/docker.sock  # Acceso a Docker para ejecutar comandos en otros contenedores
    environment:
      - AIRFLOW__CORE__EXECUTOR=LocalExecutor
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@postgres/airflow
      - AIRFLOW__CORE__FERNET_KEY=
      - AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION=true
      - AIRFLOW__CORE__LOAD_EXAMPLES=false
      - AIRFLOW__API__AUTH_BACKEND=airflow.api.auth.backend.basic_auth
    ports:
      - "8080:8080"
    command: webserver
    healthcheck:
      test: ["CMD", "curl", "--fail", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 5
    restart: unless-stopped
    networks:
      - ml-network

  # ============================================
  # Servicio: Airflow Scheduler (IMPORTANTE!)
  # ============================================
  airflow-scheduler:
    image: apache/airflow:2.7.0
    container_name: airflow-scheduler
    depends_on:
      - airflow-init
    volumes:
      - ./dags:/opt/airflow/dags
      - ./logs:/opt/airflow/logs
      - ./plugins:/opt/airflow/plugins
      - /var/run/docker.sock:/var/run/docker.sock  # Acceso a Docker para ejecutar comandos en otros contenedores
    environment:
      - AIRFLOW__CORE__EXECUTOR=LocalExecutor
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@postgres/airflow
      - AIRFLOW__CORE__FERNET_KEY=
      - AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION=true
      - AIRFLOW__CORE__LOAD_EXAMPLES=false
    command: scheduler
    healthcheck:
      test: ["CMD", "curl", "--fail", "http://localhost:8974/health"]
      interval: 30s
      timeout: 10s
      retries: 5
    restart: unless-stopped
    networks:
      - ml-network

  # ============================================
  # Servicio: Airflow Postgres
  # ============================================
  postgres:
    image: postgres:13
    container_name: postgres
    environment:
      - POSTGRES_USER=airflow
      - POSTGRES_PASSWORD=airflow
      - POSTGRES_DB=airflow
    volumes:
      - postgres-db-volume:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD", "pg_isready", "-U", "airflow"]
      interval: 5s
      retries: 5
    restart: unless-stopped
    networks:
      - ml-network

  # ============================================
  # Servicio: Airflow Init
  # ============================================
  airflow-init:
    image: apache/airflow:2.7.0
    container_name: airflow-init
    entrypoint: /bin/bash
    command:
      - -c
      - |
        function check_dbs {
          python << EOF
        import sys
        import psycopg2
        try:
            conn = psycopg2.connect("host=postgres port=5432 dbname=airflow user=airflow password=airflow")
        except psycopg2.OperationalError:
            sys.exit(1)
        sys.exit(0)
        EOF
        }
        until check_dbs; do
          echo "Waiting for postgres..."
          sleep 5
        done
        airflow db init
        airflow users create \
          --username admin \
          --firstname Admin \
          --lastname User \
          --role Admin \
          --email admin@example.com \
          --password admin
    volumes:
      - ./dags:/opt/airflow/dags
      - ./logs:/opt/airflow/logs
      - ./plugins:/opt/airflow/plugins
    environment:
      - AIRFLOW__CORE__EXECUTOR=LocalExecutor
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@postgres/airflow
      - AIRFLOW__CORE__FERNET_KEY=
      - AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION=true
      - AIRFLOW__CORE__LOAD_EXAMPLES=false
    depends_on:
      - postgres
    networks:
      - ml-network

networks:
  ml-network:
    driver: bridge

volumes:
  postgres-db-volume:
    driver: local

