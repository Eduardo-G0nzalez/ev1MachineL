"""
Script para generar reporte final de experimentos.

Genera un reporte consolidado con m√©tricas de todos los modelos,
tabla comparativa y conclusiones.

Autores: Mathias Jara & Eduardo Gonzalez
"""

import sys
from pathlib import Path
import pandas as pd
import numpy as np
from datetime import datetime

# Agregar src al path
project_path = Path(__file__).resolve().parents[1]
sys.path.insert(0, str(project_path / "src"))

def generate_classification_report():
    """Generar reporte de clasificaci√≥n"""
    
    # Cargar m√©tricas
    try:
        classification_metrics = pd.read_pickle(project_path / "data/06_models/classification_metrics.pkl")
    except FileNotFoundError:
        print("‚ö†Ô∏è Archivo de m√©tricas no encontrado")
        return None
    
    print("\n" + "="*80)
    print("üìä REPORTE DE MODELOS DE CLASIFICACI√ìN")
    print("="*80)
    print(classification_metrics.to_string(index=False))
    
    # Mejor modelo
    best_model = classification_metrics.loc[
        classification_metrics['F1_Score'].idxmax()
    ]
    
    print(f"\nüèÜ MEJOR MODELO: {best_model['Model']}")
    print(f"   F1-Score: {best_model['F1_Score']:.4f}")
    print(f"   Accuracy: {best_model['Accuracy']:.4f}")
    print(f"   CV Mean: {best_model['CV_Mean']:.4f} ¬± {best_model['CV_Std']:.4f}")
    
    return classification_metrics


def generate_regression_report():
    """Generar reporte de regresi√≥n"""
    
    # Cargar m√©tricas
    try:
        regression_metrics = pd.read_pickle(project_path / "data/06_models/regression_metrics.pkl")
    except FileNotFoundError:
        print("‚ö†Ô∏è Archivo de m√©tricas no encontrado")
        return None
    
    print("\n" + "="*80)
    print("üìä REPORTE DE MODELOS DE REGRESI√ìN")
    print("="*80)
    print(regression_metrics.to_string(index=False))
    
    # Mejor modelo (R¬≤ m√°s alto, RMSE m√°s bajo)
    best_model = regression_metrics.loc[
        regression_metrics['R2_Score'].idxmax()
    ]
    
    print(f"\nüèÜ MEJOR MODELO: {best_model['Model']}")
    print(f"   R¬≤ Score: {best_model['R2_Score']:.4f}")
    print(f"   RMSE: {best_model['RMSE']:.4f}")
    print(f"   CV RMSE Mean: {best_model['CV_RMSE_Mean']:.4f} ¬± {best_model['CV_RMSE_Std']:.4f}")
    
    return regression_metrics


def save_final_report(class_metrics, reg_metrics):
    """Guardar reporte final en markdown"""
    
    report_path = project_path / "data/07_model_output/comparison_report.md"
    report_path.parent.mkdir(parents=True, exist_ok=True)
    
    with open(report_path, 'w', encoding='utf-8') as f:
        f.write("# Reporte Final de Experimentos - Machine Learning\n\n")
        f.write(f"**Fecha**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
        f.write("---\n\n")
        
        # Clasificaci√≥n
        f.write("## üìä Modelos de Clasificaci√≥n\n\n")
        if class_metrics is not None:
            f.write(class_metrics.to_markdown(index=False))
            f.write("\n\n")
            
            best_class = class_metrics.loc[class_metrics['F1_Score'].idxmax()]
            f.write(f"**Mejor Modelo**: {best_class['Model']}\n\n")
            f.write(f"- F1-Score: {best_class['F1_Score']:.4f}\n")
            f.write(f"- CV: {best_class['CV_Mean']:.4f} ¬± {best_class['CV_Std']:.4f}\n\n")
        
        f.write("---\n\n")
        
        # Regresi√≥n
        f.write("## üìä Modelos de Regresi√≥n\n\n")
        if reg_metrics is not None:
            f.write(reg_metrics.to_markdown(index=False))
            f.write("\n\n")
            
            best_reg = reg_metrics.loc[reg_metrics['R2_Score'].idxmax()]
            f.write(f"**Mejor Modelo**: {best_reg['Model']}\n\n")
            f.write(f"- R¬≤ Score: {best_reg['R2_Score']:.4f}\n")
            f.write(f"- RMSE: {best_reg['RMSE']:.4f}\n")
            f.write(f"- CV RMSE: {best_reg['CV_RMSE_Mean']:.4f} ¬± {best_reg['CV_RMSE_Std']:.4f}\n\n")
        
        f.write("---\n\n")
        f.write("## üìù Conclusiones\n\n")
        f.write("### Clasificaci√≥n\n\n")
        f.write("- El modelo con mejor desempe√±o fue identificado mediante F1-Score y validaci√≥n cruzada.\n")
        f.write("- Se evaluaron 5 modelos diferentes con GridSearchCV para optimizaci√≥n de hiperpar√°metros.\n\n")
        
        f.write("### Regresi√≥n\n\n")
        f.write("- El modelo con mejor capacidad predictiva fue identificado mediante R¬≤ y RMSE.\n")
        f.write("- Se aplic√≥ validaci√≥n cruzada (k=5) para robustez de las m√©tricas.\n\n")
        
        f.write("---\n\n")
        f.write("*Generado autom√°ticamente por el pipeline de Kedro*\n")
        f.write(f"*Mathias Jara - Full Stack Developer*\n")
        f.write(f"*Eduardo Gonzalez - Data Scientist*\n")
    
    print(f"\n‚úÖ Reporte guardado en: {report_path}")


if __name__ == "__main__":
    print("üöÄ Generando reporte final...")
    
    class_metrics = generate_classification_report()
    reg_metrics = generate_regression_report()
    
    save_final_report(class_metrics, reg_metrics)
    
    print("\n‚úÖ Reporte final generado exitosamente")

