# Mejoras Aplicadas al Notebook de RegresiÃ³n

## ðŸš€ OBJETIVO: Elevar RÂ² de 0.28 a > 0.50

### âœ… Mejoras Implementadas

#### 1. **Feature Engineering Agresivo** (15+ nuevas features)

**Features originales mejoradas:**
- `minute_log`: Logaritmo de duraciÃ³n (normalizaciÃ³n)
- `minute_sqrt`: RaÃ­z cuadrada de duraciÃ³n
- `minute_squared`: Cuadrado de duraciÃ³n (captura no-linealidad)
- `date_squared`: Cuadrado del aÃ±o

**Features temporales:**
- `decade_encoded`: DÃ©cada codificada (2005, 2015)
- `years_from_2000`: AÃ±os transcurridos desde 2000
- `is_recent`: Binaria (1 si >= 2010, 0 si <)
- `minute_per_year`: Ratio duraciÃ³n/aÃ±o

**Features de diversidad:**
- `genre_diversity`: Cantidad de gÃ©neros por pelÃ­cula
- `duration_category`: CategorÃ­as de duraciÃ³n (1-4)
- `is_multi_genre`: Binaria (mÃ¡s de 3 gÃ©neros)
- `is_long_film`: >120 min
- `is_very_long`: >150 min
- `is_short`: <90 min

**Interacciones complejas:**
- `duration_year_interaction`: DuraciÃ³n Ã— AÃ±o
- `duration_genre_interaction`: DuraciÃ³n Ã— GÃ©neros
- `year_genre_interaction`: AÃ±o Ã— GÃ©neros
- `long_recent`: PelÃ­cula larga Y reciente
- `short_old`: PelÃ­cula corta Y antigua
- `multi_genre_recent`: Multi-gÃ©nero Y reciente

#### 2. **Modelos Mejorados**

**Antes:**
- Linear Regression, Random Forest bÃ¡sico
- HiperparÃ¡metros limitados

**Ahora:**
- Random Forest: 300-500 Ã¡rboles, max_depth 25-30
- Extra Trees: 300-500 Ã¡rboles, max_depth 25-30
- Gradient Boosting: 300-500 Ã¡rboles, max_depth 10-15, learning_rate fino
- XGBoost (si disponible): tuning agresivo + early stopping
- Ensemble final: combinaciÃ³n de mejores modelos

#### 3. **Optimizaciones de Procesamiento**

- **n_jobs=-1**: Usa todos los cores del CPU
- **tree_method='hist'** en XGBoost: MÃ¡s rÃ¡pido
- **Filtrado de outliers**: Solo pelÃ­culas 30-300 min
- **CV=5**: ValidaciÃ³n cruzada robusta

#### 4. **GridSearchCV Mejorado**

**Rangos expandidos:**
- n_estimators: [300, 500] (antes [100, 200])
- max_depth: [25, 30, None] (antes [15, 20])
- learning_rate: [0.03, 0.05, 0.1] (fino)
- subsample: [0.8, 1.0]
- colsample_bytree: [0.8, 1.0] (solo XGBoost)

---

## â±ï¸ Tiempo Estimado de EjecuciÃ³n

**ConfiguraciÃ³n actual:**
- 3-5 modelos base con GridSearch
- + XGBoost si disponible
- + Ensemble final
- **Total: 6-8 horas** (puede variar segÃºn CPU)

**Por modelo individual:**
- Random Forest/Gradient Boosting: ~2-3 horas
- Extra Trees: ~2-3 horas
- XGBoost: ~1-2 horas
- Ensemble final: ~30 min

---

## ðŸ“Š Resultados Esperados

### RÂ² Esperado: **0.50 - 0.65**
- Con las nuevas features: +0.10 a +0.15 de RÂ²
- Con modelos mÃ¡s profundos: +0.05 a +0.10 de RÂ²
- **Total esperado: ~0.48-0.65 RÂ²**

### Mejoras en RMSE
- **Antes**: ~0.35
- **Ahora esperado**: **0.28-0.32**

---

## ðŸŽ¯ CÃ³mo Usar

### OpciÃ³n 1: Jupyter (Recomendado - ver resultados en tiempo real)

```bash
cd ev1MachineL/notebooks
jupyter notebook
# Abre Fase4_Regresion.ipynb
# Ejecuta todas las celdas (Kernel â†’ Run All)
```

### OpciÃ³n 2: Docker + Airflow (Automatizado - para dejarlo toda la noche)

```bash
cd ev1MachineL

# Iniciar servicios
docker-compose up -d

# Ver logs
docker-compose logs -f kedro-pipeline

# Acceder a Airflow UI
# http://localhost:8080
```

**En Airflow:**
1. Abre `kedro_regression` DAG
2. Trigger DAG manualmente
3. Deja correr toda la noche
4. Revisa resultados al dÃ­a siguiente

---

## âš ï¸ Notas Importantes

1. **CPU Intensivo**: Los modelos usan todos los cores
2. **RAM**: NecesitarÃ¡s al menos 8GB RAM disponible
3. **Tiempo**: Dejar toda la noche es lo ideal
4. **Resultados**: Se guardan automÃ¡ticamente en `data/06_models/`

---

## ðŸ“ QuÃ© Revisar al Dia Siguiente

1. **MÃ©tricas finales en celda 11**
2. **GrÃ¡ficos de predicciones vs reales (celda 15)**
3. **Mejor modelo identificado**
4. **RÂ² esperado: > 0.50**

Â¡Listo para dejar corriendo toda la noche! ðŸŒ™



